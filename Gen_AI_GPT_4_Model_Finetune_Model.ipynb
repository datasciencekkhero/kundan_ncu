{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IggYrMrvecfS",
    "outputId": "edf63105-90df-49be-e4ff-72ef71c34b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hGkenBIgedil"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Google Drive into a pandas DataFrame\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PhD_Thesis_Experiments/GitHub_ToChair/sample_complaints_2years_006_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "vjLNlj0zrB91"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "INNZnX1Hvd_M"
   },
   "outputs": [],
   "source": [
    "# ==================== Configuration ====================\n",
    "AZURE_OPENAI_ENDPOINT = \"https://datascienceagenticaiwork.cognitiveservices.azure.com/\"\n",
    "AZURE_OPENAI_KEY = \"\"\n",
    "AZURE_OPENAI_API_VERSION = \"2024-12-01-preview\"  # Use appropriate API version\n",
    "BASE_MODEL = \"gpt-4o-mini\"  # Base model for fine-tuning\n",
    "deployment = \"gpt-4o-mini-tobefinetune\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_1Gck7OMwVPt"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to split hierarchical labels into product and sub-product\n",
    "def split_hierarchical_label(label):\n",
    "    if '::' in label:\n",
    "        return label.split('::')\n",
    "    else:\n",
    "        return [label, 'None'] # Handle cases with no sub-product\n",
    "\n",
    "# Function to calculate hierarchical metrics\n",
    "def hierarchical_metrics(y_true, y_pred):\n",
    "    product_true = [split_hierarchical_label(label)[0] for label in y_true]\n",
    "    sub_product_true = [split_hierarchical_label(label)[1] for label in y_true]\n",
    "    product_pred = [split_hierarchical_label(label)[0] for label in y_pred]\n",
    "    sub_product_pred = [split_hierarchical_label(label)[1] for label in y_pred]\n",
    "\n",
    "    # Calculate metrics at the product level\n",
    "    product_precision = precision_score(product_true, product_pred, average='weighted', zero_division=0)\n",
    "    product_recall = recall_score(product_true, product_pred, average='weighted', zero_division=0)\n",
    "    product_f1 = f1_score(product_true, product_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate metrics at the sub-product level (only for non-None sub-products)\n",
    "    # We need to filter for cases where both true and predicted sub-products are not 'None'\n",
    "    valid_sub_product_true = [sub for i, sub in enumerate(sub_product_true) if sub != 'None' and sub_product_pred[i] != 'None']\n",
    "    valid_sub_product_pred = [sub for i, sub in enumerate(sub_product_pred) if sub != 'None' and sub_product_true[i] != 'None']\n",
    "\n",
    "\n",
    "    sub_product_precision = precision_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n",
    "    sub_product_recall = recall_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n",
    "    sub_product_f1 = f1_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n",
    "\n",
    "\n",
    "    # A simple way to combine scores (can be weighted based on importance)\n",
    "    # Here, we'll just average them\n",
    "    hierarchical_precision = (product_precision + sub_product_precision) / 2\n",
    "    hierarchical_recall = (product_recall + sub_product_recall) / 2\n",
    "    hierarchical_f1 = (product_f1 + sub_product_f1) / 2\n",
    "\n",
    "    return {\n",
    "        'product_precision': product_precision,\n",
    "        'product_recall': product_recall,\n",
    "        'product_f1': product_f1,\n",
    "        'sub_product_precision': sub_product_precision,\n",
    "        'sub_product_recall': sub_product_recall,\n",
    "        'sub_product_f1': sub_product_f1,\n",
    "        'hierarchical_precision': hierarchical_precision,\n",
    "        'hierarchical_recall': hierarchical_recall,\n",
    "        'hierarchical_f1': hierarchical_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "74psj2E2un9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6T9C1OjwtSr",
    "outputId": "14764ee0-f1d0-4c7b-9194-33beb62cbe92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 14400\n",
      "Validation samples: 1600\n",
      "‚úÖ File saved: fine_tune_data/train_data.jsonl (14400 records)\n",
      "‚úÖ File saved: fine_tune_data/val_data.jsonl (1600 records)\n",
      "üéØ Training and validation JSONL files are ready for Azure OpenAI fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ Load or use your dataframe\n",
    "# ------------------------------\n",
    "# If already in memory: df = your DataFrame\n",
    "# Example if loading from CSV:\n",
    "# df = pd.read_csv(\"complaints.csv\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert {\"consumer_complaint_narrative\", \"hierarchical_label\"}.issubset(df.columns), \\\n",
    "    \"DataFrame must contain 'consumer_complaint_narrative' and 'hierarchical_label' columns.\"\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=[\"consumer_complaint_narrative\", \"hierarchical_label\"])\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ Split into train & validation\n",
    "# ------------------------------\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"hierarchical_label\"])\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ Helper function to save JSONL\n",
    "# ------------------------------\n",
    "def save_jsonl(dataframe, output_path):\n",
    "    \"\"\"Save dataframe to Azure OpenAI fine-tuning JSONL format.\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            complaint_text = str(row[\"consumer_complaint_narrative\"]).replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "            label = str(row[\"hierarchical_label\"]).strip()\n",
    "\n",
    "            record = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that classifies consumer financial complaints into categories.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": complaint_text\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": label\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ File saved: {output_path} ({len(dataframe)} records)\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4Ô∏è‚É£ Create output directory & save\n",
    "# ------------------------------\n",
    "os.makedirs(\"fine_tune_data\", exist_ok=True)\n",
    "\n",
    "save_jsonl(train_df, \"fine_tune_data/train_data.jsonl\")\n",
    "save_jsonl(val_df, \"fine_tune_data/val_data.jsonl\")\n",
    "\n",
    "print(\"üéØ Training and validation JSONL files are ready for Azure OpenAI fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UG0Xf2bGBqEn"
   },
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAbQO-BuwtP_",
    "outputId": "9bdda1db-0b3f-434c-ab84-576cd637b924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file ID: file-ac2162910b0442979cf67e1323c8cd0a\n",
      "Validation file ID: file-77ec932e57444752af3b0eae4524f0e2\n"
     ]
    }
   ],
   "source": [
    "# Upload files directly from Python\n",
    "with open(\"fine_tune_data/train_data.jsonl\", \"rb\") as f:\n",
    "    train_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "with open(\"fine_tune_data/val_data.jsonl\", \"rb\") as f:\n",
    "    val_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(\"Train file ID:\", train_file.id)\n",
    "print(\"Validation file ID:\", val_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ykriXqkDeiZ"
   },
   "source": [
    "### Full Supervised Fine‚ÄëTuning (SFT) on Azure OpenAI GPT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKxhycTgw9mR",
    "outputId": "8c86b442-1880-4302-a0e7-2395c5f8ce6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuning job started!\n",
      "Job ID: ftjob-36293937da9a4d58bc21d14dcf13f813\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Continue using the same client and configuration from before\n",
    "fine_tune = client.fine_tuning.jobs.create(\n",
    "    model = \"gpt-4o-2024-08-06\",   # BASE_MODEL,  # \"gpt-4.1\" or \"gpt-4.1-mini\"\n",
    "    training_file=\"file-ac2162910b0442979cf67e1323c8cd0a\",\n",
    "    validation_file=\"file-77ec932e57444752af3b0eae4524f0e2\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 3  # you can tune this; 2‚Äì4 is typical\n",
    "    },\n",
    "    suffix=\"cfpb-complaint-classifier\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fine-tuning job started!\")\n",
    "print(\"Job ID:\", fine_tune.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIJ8qVWnw9gD",
    "outputId": "da5e0c1a-6c50-4cd8-8d55-9f0a63db9791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: succeeded\n"
     ]
    }
   ],
   "source": [
    "status = client.fine_tuning.jobs.retrieve(\"ftjob-36293937da9a4d58bc21d14dcf13f813\")\n",
    "print(\"Status:\", status.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P1o4Y6DDGyh"
   },
   "source": [
    "### Get the fine‚Äëtuned model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnYYFh4mwtKy",
    "outputId": "16aee10b-e52c-4946-840f-0e0b1b6360db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: succeeded\n",
      "Fine-tuned model ID: gpt-4o-2024-08-06.ft-36293937da9a4d58bc21d14dcf13f813-cfpb-complaint-classifier\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "job_id = \"ftjob-36293937da9a4d58bc21d14dcf13f813\"\n",
    "job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(\"Status:\", job.status)\n",
    "print(\"Fine-tuned model ID:\", job.fine_tuned_model)\n",
    "\n",
    "FT_MODEL_ID = job.fine_tuned_model  # e.g., 'ft:gpt-4o-2024-08-06:complaints-v1-2025-11-05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1jcAtx5Eyto"
   },
   "source": [
    "### Using Azure Portal deployed the Fine-tuned model ID: gpt-4o-2024-08-06.ft-36293937da9a4d58bc21d14dcf13f813-cfpb-complaint-classifier with name:  gpt-4o-2024-08-06-custom-multiclass-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "86GUMFG4wbXk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import random\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm import tqdm\n",
    "from openai import AsyncAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oWsqANRJwbVQ"
   },
   "outputs": [],
   "source": [
    "#DEPLOYMENT_NAME = \"gpt-4o-2024-08-06-custom-multiclass-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "t3_Pxmw9l-Bv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "81bK1WGzl9_V"
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 2Ô∏è‚É£ Dataset Setup\n",
    "# ==========================================================\n",
    "# Example: df has ['consumer_complaint_narrative', 'hierarchical_label']\n",
    "# df = pd.read_csv('cfpb_complaints.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['hierarchical_label'])\n",
    "unique_labels = df['hierarchical_label'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0IOJmb-_l952"
   },
   "outputs": [],
   "source": [
    "AZURE_API_KEY = \"\"\n",
    "AZURE_ENDPOINT = \"https://datascienceagenticaiwork.cognitiveservices.azure.com/\"\n",
    "API_VERSION = \"2024-12-01-preview\"\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "DEPLOYMENT_NAME = \"gpt-4o-2024-08-06-custom-multiclass-classifier\"\n",
    "\n",
    "# -----------------------------\n",
    "# INITIALIZE CLIENT\n",
    "# -----------------------------\n",
    "# Use AsyncAzureOpenAI for asynchronous operations\n",
    "aclient = AsyncAzureOpenAI(\n",
    "    api_key=AZURE_API_KEY,\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=AZURE_ENDPOINT\n",
    ")\n",
    "\n",
    "# --- Set your Azure OpenAI credentials and endpoint ---\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_API_KEY  # <-- Put your key (or store securely)\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_ENDPOINT # <-- Your endpoint\n",
    "\n",
    "# Create the Azure OpenAI client (use the API version supported by your resource)\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version = API_VERSION,\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    ")\n",
    "\n",
    "# Use your **deployment name** (not the base model name)\n",
    "# deployment_name = MODEL_NAME  # e.g., \"gpt4o-mini-prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aRr-Fr7kl93N"
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 4Ô∏è‚É£ Async GPT-4.1 Zero-Shot Classifier\n",
    "# ==========================================================\n",
    "import asyncio\n",
    "import random\n",
    "\n",
    "async def classify_with_gpt40(session, text, label_list, max_retries=3):\n",
    "    prompt = f\"\"\"\n",
    "    You are a financial complaint classifier.\n",
    "    Given the following consumer complaint, classify it into one of these hierarchical categories:\n",
    "    {', '.join(label_list)}\n",
    "\n",
    "    Complaint:\n",
    "    \"{text}\"\n",
    "\n",
    "    Respond with exactly one label from the list above.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{AZURE_ENDPOINT}openai/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": AZURE_API_KEY}\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with session.post(url, headers=headers, json=payload, timeout=60) as resp:\n",
    "                if resp.status == 200:\n",
    "                    data = await resp.json()\n",
    "                    return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "                # Handle 429 (Rate Limit)\n",
    "                elif resp.status == 429:\n",
    "                    await asyncio.sleep(1 + random.random())  # small random delay\n",
    "                    continue  # retry silently\n",
    "\n",
    "                # Handle transient server errors quietly\n",
    "                elif 500 <= resp.status < 600:\n",
    "                    await asyncio.sleep(1 + random.random())\n",
    "                    continue  # retry silently\n",
    "\n",
    "                # For other non-critical errors: skip quietly\n",
    "                else:\n",
    "                    return \"\"\n",
    "\n",
    "        except Exception:\n",
    "            # Suppress exceptions silently, retry\n",
    "            await asyncio.sleep(0.5)\n",
    "            continue\n",
    "\n",
    "    # Return empty label if all retries fail\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rKIFCV7Kl90c"
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 5Ô∏è‚É£ Parallel Inference\n",
    "# ==========================================================\n",
    "async def run_parallel_inference(test_data, label_list, max_concurrent=5):\n",
    "    y_true, y_pred = [], []\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "\n",
    "        async def process_row(row):\n",
    "            async with semaphore:\n",
    "                predicted = await classify_with_gpt40(session, row['consumer_complaint_narrative'], label_list)\n",
    "                return row['hierarchical_label'], predicted\n",
    "\n",
    "        for _, row in test_data.iterrows():\n",
    "            tasks.append(process_row(row))\n",
    "\n",
    "        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Classifying\", leave=False):\n",
    "            true_label, pred_label = await f\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(pred_label)\n",
    "\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2U0AIzul9xk",
    "outputId": "37995e67-5341-4457-9cab-e4a7ec96af33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 6Ô∏è‚É£ Main Execution\n",
    "# ==========================================================\n",
    "# y_true, y_pred = asyncio.run(run_parallel_inference(test_df, unique_labels, max_concurrent=5))\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()  # allows reusing event loop inside notebook\n",
    "\n",
    "# Run async inference\n",
    "y_true, y_pred = await run_parallel_inference(test_df, unique_labels, max_concurrent=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dTy-Bs1l9uU",
    "outputId": "ac8f0d23-bf97-4ecb-f163-ff9597e6d656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Precision: 0.7702\n",
      "Product Recall: 0.6328\n",
      "Product F1: 0.6798\n",
      "Sub Product Precision: 0.6596\n",
      "Sub Product Recall: 0.5737\n",
      "Sub Product F1: 0.5621\n",
      "Hierarchical Precision: 0.7149\n",
      "Hierarchical Recall: 0.6033\n",
      "Hierarchical F1: 0.6210\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 7Ô∏è‚É£ Evaluate Results\n",
    "# ==========================================================\n",
    "metrics = hierarchical_metrics(y_true, y_pred)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuOcHKyQl9rN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmc5e1ZlwbAx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
