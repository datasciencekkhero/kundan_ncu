{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPx0X8eVBQ7pmL4aNv+jn6V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e6a33cd7eaae41158af7659a6fe21398":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bbecbe0d7a94c1a8efe3b2fefecf4eb","IPY_MODEL_4d24a7210677463fb505653078de3f01","IPY_MODEL_413b76d9dafd401fb83e545a9ee13e1a"],"layout":"IPY_MODEL_133253024f90460d8e33f285fe710334"}},"9bbecbe0d7a94c1a8efe3b2fefecf4eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe3ac2d19a644434ae555da88d6e7f45","placeholder":"​","style":"IPY_MODEL_b0821a1b57694ba290cd7e0b31e9aa19","value":"tokenizer_config.json: 100%"}},"4d24a7210677463fb505653078de3f01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a0ee8be5aa24efda2a5b2a8ad03fc4e","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4aa22ac13d542fd9c9d8696be667480","value":26}},"413b76d9dafd401fb83e545a9ee13e1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9a3b4a0d4744554bdfd410fd7bde02e","placeholder":"​","style":"IPY_MODEL_cdb6fc5244f044a3bbe8aef47c686bb1","value":" 26.0/26.0 [00:00&lt;00:00, 294B/s]"}},"133253024f90460d8e33f285fe710334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe3ac2d19a644434ae555da88d6e7f45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0821a1b57694ba290cd7e0b31e9aa19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a0ee8be5aa24efda2a5b2a8ad03fc4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4aa22ac13d542fd9c9d8696be667480":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9a3b4a0d4744554bdfd410fd7bde02e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdb6fc5244f044a3bbe8aef47c686bb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dea30ad663f94599be4d31f74b121a2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76c7add27b7c48c79f492fc6faf80aa4","IPY_MODEL_33836de09a2e4a9ca3f5638fdec3d507","IPY_MODEL_e0b3b232aec648fdbe3c73c91c11a5b6"],"layout":"IPY_MODEL_981bb55f2f524c9d883317c81e3a7e8a"}},"76c7add27b7c48c79f492fc6faf80aa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4281de7adac4751bca14cd31309cda8","placeholder":"​","style":"IPY_MODEL_73ab33fd5bf74b89869e903992ea6949","value":"vocab.json: 100%"}},"33836de09a2e4a9ca3f5638fdec3d507":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fc1b8fa5d3a4a8c91493bbe1b540755","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9477df8e63c43e7a71ed3cf3d01f3b6","value":1042301}},"e0b3b232aec648fdbe3c73c91c11a5b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33b835b934e4a1a943980cd38f52af4","placeholder":"​","style":"IPY_MODEL_a46a9c89c4cd4b1da7eb849d987c9250","value":" 1.04M/1.04M [00:00&lt;00:00, 4.17MB/s]"}},"981bb55f2f524c9d883317c81e3a7e8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4281de7adac4751bca14cd31309cda8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73ab33fd5bf74b89869e903992ea6949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fc1b8fa5d3a4a8c91493bbe1b540755":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9477df8e63c43e7a71ed3cf3d01f3b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c33b835b934e4a1a943980cd38f52af4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46a9c89c4cd4b1da7eb849d987c9250":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb8acef63acb4ca9879e0e7da0f702a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab3726d5ad9a4b3381a45c2dfb3fbbb8","IPY_MODEL_c80e13758e1547768b7f3ad9b44b0373","IPY_MODEL_6717d1476acc49f39e3b84afcf3696a4"],"layout":"IPY_MODEL_e460e4fdf29d4993af71c22e2cfafc0d"}},"ab3726d5ad9a4b3381a45c2dfb3fbbb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_240fe4c0de7d4072bedcb87c0ee18dd4","placeholder":"​","style":"IPY_MODEL_13c9e794bd7648e687f1a40b6e455680","value":"merges.txt: 100%"}},"c80e13758e1547768b7f3ad9b44b0373":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a98a4961a60e4bedaac38e9269aa105a","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_973f221d241d49f5a1e19e294d7890fd","value":456318}},"6717d1476acc49f39e3b84afcf3696a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ab3eb93d7ba4f738fb74db85a557c1c","placeholder":"​","style":"IPY_MODEL_7b585cec79c042c6b5f5006945e4b906","value":" 456k/456k [00:00&lt;00:00, 3.66MB/s]"}},"e460e4fdf29d4993af71c22e2cfafc0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"240fe4c0de7d4072bedcb87c0ee18dd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13c9e794bd7648e687f1a40b6e455680":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a98a4961a60e4bedaac38e9269aa105a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"973f221d241d49f5a1e19e294d7890fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ab3eb93d7ba4f738fb74db85a557c1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b585cec79c042c6b5f5006945e4b906":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"972ab552be514118ada80ee196c5717c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3dd0c5896d5f429ea92cb4f678073f27","IPY_MODEL_37ce8a7d44304c75b43c31b077ff76d7","IPY_MODEL_3d111135ef7d46b6959b85cbd6b8e4f0"],"layout":"IPY_MODEL_3cd8ba6a2dd545ad966c53326de0720f"}},"3dd0c5896d5f429ea92cb4f678073f27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50eaa21fd4e14a069b7663519e88f644","placeholder":"​","style":"IPY_MODEL_8343c08eb6754a9e89cd773a04232d9b","value":"tokenizer.json: 100%"}},"37ce8a7d44304c75b43c31b077ff76d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5a001a3fcf344d0bc5886c26cb2111a","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_356b3da2f78b4e59a883be10140e4e1b","value":1355256}},"3d111135ef7d46b6959b85cbd6b8e4f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28c971a2d51044f9b35ff2195fef1f7f","placeholder":"​","style":"IPY_MODEL_0905890f391849179eeba793ff1bd64a","value":" 1.36M/1.36M [00:00&lt;00:00, 4.31MB/s]"}},"3cd8ba6a2dd545ad966c53326de0720f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50eaa21fd4e14a069b7663519e88f644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8343c08eb6754a9e89cd773a04232d9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5a001a3fcf344d0bc5886c26cb2111a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"356b3da2f78b4e59a883be10140e4e1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28c971a2d51044f9b35ff2195fef1f7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0905890f391849179eeba793ff1bd64a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"065add95131d4cb0a3c7a84fce1fc2ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ae1cb7874be466ea7f2ee1cf3af3de0","IPY_MODEL_2589786454324cc3a0f5dca1614d08e3","IPY_MODEL_356d4055139d46e589f2ed0dc6b69a92"],"layout":"IPY_MODEL_9087a416873a46efb59ba2473c09b6bc"}},"4ae1cb7874be466ea7f2ee1cf3af3de0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f18b461e0ca24cb1a028692e6902b80e","placeholder":"​","style":"IPY_MODEL_0acf382d580d41e1b7cb7f1950ee3549","value":"config.json: 100%"}},"2589786454324cc3a0f5dca1614d08e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5b368967a5e4da398ae08b01bcb6478","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_020534237a814ebf903cbbad8041ae43","value":665}},"356d4055139d46e589f2ed0dc6b69a92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60add58d51174a799b89231e5b711705","placeholder":"​","style":"IPY_MODEL_af37ce4e207b43bcaaf26de31c7b0da4","value":" 665/665 [00:00&lt;00:00, 9.54kB/s]"}},"9087a416873a46efb59ba2473c09b6bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f18b461e0ca24cb1a028692e6902b80e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0acf382d580d41e1b7cb7f1950ee3549":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5b368967a5e4da398ae08b01bcb6478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"020534237a814ebf903cbbad8041ae43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60add58d51174a799b89231e5b711705":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af37ce4e207b43bcaaf26de31c7b0da4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4949a06dddb54564ac0011933325aa41":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_619feb8162cd43d59d10f774f1521268","IPY_MODEL_fb0989d125af46f89bce3fce5496bcec","IPY_MODEL_7b8e7ea983504171a5a4fe070eb320df"],"layout":"IPY_MODEL_b3ff300e9b3640b9b38aeaaa3557fc96"}},"619feb8162cd43d59d10f774f1521268":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a4b5195693c4ff39dccc47c6cd798e0","placeholder":"​","style":"IPY_MODEL_125366fa88084b75aa3ca423230266cb","value":"Map: 100%"}},"fb0989d125af46f89bce3fce5496bcec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65afebce24ae45449444e1c488d01353","max":86400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c77039d803543b9b4ddb539b0d0bc17","value":86400}},"7b8e7ea983504171a5a4fe070eb320df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e04292c153314398913ca7963f32b695","placeholder":"​","style":"IPY_MODEL_2841305870fc4bfea30bfe5809333ca8","value":" 86400/86400 [02:26&lt;00:00, 1002.01 examples/s]"}},"b3ff300e9b3640b9b38aeaaa3557fc96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a4b5195693c4ff39dccc47c6cd798e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125366fa88084b75aa3ca423230266cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65afebce24ae45449444e1c488d01353":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c77039d803543b9b4ddb539b0d0bc17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e04292c153314398913ca7963f32b695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2841305870fc4bfea30bfe5809333ca8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a0efefda895467c90a6795e8073d73e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b27e9f3ca6e04253bd436cffbb80a2bb","IPY_MODEL_d9ef7d8b63bb43f194cffe9e4e06ce41","IPY_MODEL_84498bba11c64ceba53ef02cdf8a4f2c"],"layout":"IPY_MODEL_5fd71079058149ed90d64197c57b36d7"}},"b27e9f3ca6e04253bd436cffbb80a2bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70d29925e11341169bff92d57b7e99be","placeholder":"​","style":"IPY_MODEL_3d59727611f84c1babac815c7847c6ad","value":"Map: 100%"}},"d9ef7d8b63bb43f194cffe9e4e06ce41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93bb00c0b5a141a98b0fd6cc54692dc5","max":21600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1718d486847a45f39a07dff46b02e2fb","value":21600}},"84498bba11c64ceba53ef02cdf8a4f2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e68462735104ed6a7b0c4c16030a6ea","placeholder":"​","style":"IPY_MODEL_35d60a1253ca4428992be1b0929e9c41","value":" 21600/21600 [00:26&lt;00:00, 943.28 examples/s]"}},"5fd71079058149ed90d64197c57b36d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70d29925e11341169bff92d57b7e99be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d59727611f84c1babac815c7847c6ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93bb00c0b5a141a98b0fd6cc54692dc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1718d486847a45f39a07dff46b02e2fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e68462735104ed6a7b0c4c16030a6ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35d60a1253ca4428992be1b0929e9c41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c50d52540fd5479d9724ee5e387ba3a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f88b86a07d844e0aad170316ac49400d","IPY_MODEL_ddebe51be0954aa2896534af9123e949","IPY_MODEL_c0f61a06773d41c3ac39f29c3b6f7549"],"layout":"IPY_MODEL_d8b2b3a4da0041078fed1538bc12e367"}},"f88b86a07d844e0aad170316ac49400d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e208a4a20954170b6d7bc65c472b11a","placeholder":"​","style":"IPY_MODEL_4d8cdc0de9374e5a8de4d753b359a3c5","value":"model.safetensors: 100%"}},"ddebe51be0954aa2896534af9123e949":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c5f6f29478f418abcf93a96e1385584","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f9fedeb5bbf4d29a5ce1b1bd89644ef","value":548105171}},"c0f61a06773d41c3ac39f29c3b6f7549":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3004d4a8671e4963b0b1e52e42734e50","placeholder":"​","style":"IPY_MODEL_577f6976c84a45b3a5038f2e7c56a99f","value":" 548M/548M [00:05&lt;00:00, 111MB/s]"}},"d8b2b3a4da0041078fed1538bc12e367":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e208a4a20954170b6d7bc65c472b11a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d8cdc0de9374e5a8de4d753b359a3c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c5f6f29478f418abcf93a96e1385584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f9fedeb5bbf4d29a5ce1b1bd89644ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3004d4a8671e4963b0b1e52e42734e50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"577f6976c84a45b3a5038f2e7c56a99f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db2a996df3f24ef5ba3059e4d7fe72cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef2857e0784045a6aa28dc74a3f34da2","IPY_MODEL_ae3b337aa213415196c1b9e40b39f183","IPY_MODEL_6ce31fa3493f4468aa3efa926b692b8c"],"layout":"IPY_MODEL_27f58541371a4f4d9a8f41997e829121"}},"ef2857e0784045a6aa28dc74a3f34da2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3902881f89d745c7b3292762c48b2f4b","placeholder":"​","style":"IPY_MODEL_58b105175e02471c9eadb25291b3412f","value":"Map: 100%"}},"ae3b337aa213415196c1b9e40b39f183":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d8057e215e94042a6859508eb4bb3a9","max":86400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c2f3b2300cb450ba4a0358419c9a0f6","value":86400}},"6ce31fa3493f4468aa3efa926b692b8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bad45fcb81f463aa074fe79faa0ed13","placeholder":"​","style":"IPY_MODEL_47d0f039535342bea42f492a4401edd2","value":" 86400/86400 [01:37&lt;00:00, 1000.17 examples/s]"}},"27f58541371a4f4d9a8f41997e829121":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3902881f89d745c7b3292762c48b2f4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b105175e02471c9eadb25291b3412f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d8057e215e94042a6859508eb4bb3a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c2f3b2300cb450ba4a0358419c9a0f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bad45fcb81f463aa074fe79faa0ed13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47d0f039535342bea42f492a4401edd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91089f50b21a49e881a2e4871ec07764":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5719f7b6066344bba4434b06ef07efc0","IPY_MODEL_044cde722575441688f5640add7a5b15","IPY_MODEL_d59729e5fba341de8779eff92d28254f"],"layout":"IPY_MODEL_837d8db126a54e95ab0c4caaf356f712"}},"5719f7b6066344bba4434b06ef07efc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01e8542df9584a00bfc360fbe1f0af86","placeholder":"​","style":"IPY_MODEL_26438d2c67d74f48b20846ca44a7ddb6","value":"Map: 100%"}},"044cde722575441688f5640add7a5b15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e84a1e6e5a024eb28bcc35302fddb400","max":21600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7dd55f0501cd4aedb2f91270cce07106","value":21600}},"d59729e5fba341de8779eff92d28254f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8fec893f3e2488a8de1cf8047ff46d5","placeholder":"​","style":"IPY_MODEL_106ca99b75b44af9919b8311c6759726","value":" 21600/21600 [00:24&lt;00:00, 829.01 examples/s]"}},"837d8db126a54e95ab0c4caaf356f712":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01e8542df9584a00bfc360fbe1f0af86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26438d2c67d74f48b20846ca44a7ddb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e84a1e6e5a024eb28bcc35302fddb400":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dd55f0501cd4aedb2f91270cce07106":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8fec893f3e2488a8de1cf8047ff46d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"106ca99b75b44af9919b8311c6759726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53f34a3746bd4e29ac1e5bc996b51cd9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd09ff1fe1dc454396b0a6ce97ed2f54","IPY_MODEL_f9e09004eb094ca28558a4eea3416ddd","IPY_MODEL_fee92aefa96640e7b44f1d808bf0a616"],"layout":"IPY_MODEL_dcfb6bc55b6e4ba1bb4d834ba58e28a5"}},"bd09ff1fe1dc454396b0a6ce97ed2f54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d831241188ec4ecca7a0f09851ab350c","placeholder":"​","style":"IPY_MODEL_b3c1a294584143828186afefce621340","value":""}},"f9e09004eb094ca28558a4eea3416ddd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36e32b00dfc34d54ab0e38e625efba4e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4512ab105e104fc4b560d407351e61df","value":0}},"fee92aefa96640e7b44f1d808bf0a616":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b40a00f1ff9941608f4ea3988982f37b","placeholder":"​","style":"IPY_MODEL_61e0dcbc49894c2798bf788377933a11","value":" 0/0 [00:00&lt;?, ?it/s]"}},"dcfb6bc55b6e4ba1bb4d834ba58e28a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d831241188ec4ecca7a0f09851ab350c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3c1a294584143828186afefce621340":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36e32b00dfc34d54ab0e38e625efba4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4512ab105e104fc4b560d407351e61df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b40a00f1ff9941608f4ea3988982f37b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61e0dcbc49894c2798bf788377933a11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"097d5018cf534d9c86d4f546b305134c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c81e88a04984dacba64154eb88c35e3","IPY_MODEL_8bb2ed78739e4493be67d62a4a06ed4e","IPY_MODEL_0eb2122bb222433fbd21b4bddcde2312"],"layout":"IPY_MODEL_c1bcb985e325491cbf1d30c93c6ed2c3"}},"9c81e88a04984dacba64154eb88c35e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f736f2ce6b2404fa0b3d73824b182b5","placeholder":"​","style":"IPY_MODEL_802942b3aa7942fc90d50ee85b06f89f","value":"Map: 100%"}},"8bb2ed78739e4493be67d62a4a06ed4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b05e5b963e7433895058ba6584a90d9","max":86400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af33cbe416484b8091dd044e07a18e01","value":86400}},"0eb2122bb222433fbd21b4bddcde2312":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e987b18b42a24106be1b027224e4d078","placeholder":"​","style":"IPY_MODEL_1ac9cb0f813b4d99b773f0c040b3abad","value":" 86400/86400 [00:27&lt;00:00, 3048.66 examples/s]"}},"c1bcb985e325491cbf1d30c93c6ed2c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f736f2ce6b2404fa0b3d73824b182b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"802942b3aa7942fc90d50ee85b06f89f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b05e5b963e7433895058ba6584a90d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af33cbe416484b8091dd044e07a18e01":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e987b18b42a24106be1b027224e4d078":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac9cb0f813b4d99b773f0c040b3abad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a2e3cfcca824e6a9516df7c1d50a7ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_135272068ab9410ea6df4e1abe75b3ff","IPY_MODEL_a4986573ebaf4aac9f89500bae4b7d37","IPY_MODEL_08cec16073664abba7974845f5ecf039"],"layout":"IPY_MODEL_a511a3a2253648e49a782b2555b86a46"}},"135272068ab9410ea6df4e1abe75b3ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1988073f251b49beadd4ca41f2803dbb","placeholder":"​","style":"IPY_MODEL_40160661f42a419ba28ca8aafb31f12e","value":"Map: 100%"}},"a4986573ebaf4aac9f89500bae4b7d37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebfe7cd60c9442c0b79e1e2503f69ede","max":21600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79adc24473184ce5820a634d36dbcc99","value":21600}},"08cec16073664abba7974845f5ecf039":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a55a9b6488a4729bc498238c67714ec","placeholder":"​","style":"IPY_MODEL_a7280092f81e427a874db17d87ac0831","value":" 21600/21600 [00:06&lt;00:00, 3045.22 examples/s]"}},"a511a3a2253648e49a782b2555b86a46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1988073f251b49beadd4ca41f2803dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40160661f42a419ba28ca8aafb31f12e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebfe7cd60c9442c0b79e1e2503f69ede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79adc24473184ce5820a634d36dbcc99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a55a9b6488a4729bc498238c67714ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7280092f81e427a874db17d87ac0831":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d8d413ef023420bb4da14aa93f0b76b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c7af6eb0c174d92a8f9a34ba1b895be","IPY_MODEL_bb9cdd85bd2c4fd387f436f954b0a3ea","IPY_MODEL_5b96849fe1824679924e6f38a53edc6a"],"layout":"IPY_MODEL_32fc3c4d81414291bde3cda462d2bb7e"}},"2c7af6eb0c174d92a8f9a34ba1b895be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_745ed74945b04024826c41b2934a957f","placeholder":"​","style":"IPY_MODEL_1137cc0cb8824800a94f925f1d531c70","value":"Map: 100%"}},"bb9cdd85bd2c4fd387f436f954b0a3ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d67ceea3b5a4578915307610fc0ab23","max":86400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a352ab13124346148c22e5c023b101e0","value":86400}},"5b96849fe1824679924e6f38a53edc6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c36645bdb044623b6ab090d90347569","placeholder":"​","style":"IPY_MODEL_4cbc5c31d83b4d5696389cec4c541364","value":" 86400/86400 [00:27&lt;00:00, 3297.38 examples/s]"}},"32fc3c4d81414291bde3cda462d2bb7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"745ed74945b04024826c41b2934a957f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1137cc0cb8824800a94f925f1d531c70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d67ceea3b5a4578915307610fc0ab23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a352ab13124346148c22e5c023b101e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c36645bdb044623b6ab090d90347569":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cbc5c31d83b4d5696389cec4c541364":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd32bde8e284460596b71808fcf5f979":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e7a800eb2d4418989cf8af405252a84","IPY_MODEL_a9556caa97e24be1ada1657b29f1afab","IPY_MODEL_654953f8fc464578b4d8cfa479c0c9e8"],"layout":"IPY_MODEL_84e5eddb05264311a0ad031d620e469e"}},"9e7a800eb2d4418989cf8af405252a84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_408074174d064a3e8277acaa8165926d","placeholder":"​","style":"IPY_MODEL_c102fabeafce4b2180093f4e8d09f260","value":"Map: 100%"}},"a9556caa97e24be1ada1657b29f1afab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdb3c152344a4c2fa3a291e12435f301","max":21600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5231bc3a689b4dde9fa71174b4eb8567","value":21600}},"654953f8fc464578b4d8cfa479c0c9e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e8a6ebd907444938ff136d1595f2c35","placeholder":"​","style":"IPY_MODEL_9cc042cd702c445cbfd1083a827a9e62","value":" 21600/21600 [00:07&lt;00:00, 3069.77 examples/s]"}},"84e5eddb05264311a0ad031d620e469e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"408074174d064a3e8277acaa8165926d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c102fabeafce4b2180093f4e8d09f260":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdb3c152344a4c2fa3a291e12435f301":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5231bc3a689b4dde9fa71174b4eb8567":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e8a6ebd907444938ff136d1595f2c35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cc042cd702c445cbfd1083a827a9e62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vUt0Cixh6USh"},"outputs":[],"source":[]},{"cell_type":"code","source":["\n","# ================================\n","# 1) Install and Imports (Colab)\n","# ================================\n","# !pip -q install --upgrade transformers datasets peft accelerate scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ErQAbVs69KS","executionInfo":{"status":"ok","timestamp":1764964378379,"user_tz":480,"elapsed":21668,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"ea75d43e-5763-4e52-84e8-42ad132682ee"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["\n","!pip -q install \"transformers==4.44.2\" \"datasets==2.21.0\" \"peft==0.10.0\" \"accelerate==0.34.2\" \"scikit-learn==1.5.2\"\n","\n","import torch, transformers, peft, datasets, sklearn\n","print(\"Torch:\", torch.__version__)\n","print(\"Transformers:\", transformers.__version__)\n","print(\"PEFT:\", peft.__version__)\n","print\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347,"referenced_widgets":["53f34a3746bd4e29ac1e5bc996b51cd9","bd09ff1fe1dc454396b0a6ce97ed2f54","f9e09004eb094ca28558a4eea3416ddd","fee92aefa96640e7b44f1d808bf0a616","dcfb6bc55b6e4ba1bb4d834ba58e28a5","d831241188ec4ecca7a0f09851ab350c","b3c1a294584143828186afefce621340","36e32b00dfc34d54ab0e38e625efba4e","4512ab105e104fc4b560d407351e61df","b40a00f1ff9941608f4ea3988982f37b","61e0dcbc49894c2798bf788377933a11"]},"id":"5XGsQXUAGq4d","executionInfo":{"status":"ok","timestamp":1764964980585,"user_tz":480,"elapsed":19376,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"f21eceff-8725-4712-ae7a-377799be36d7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n","umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53f34a3746bd4e29ac1e5bc996b51cd9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Torch: 2.9.0+cu126\n","Transformers: 4.44.2\n","PEFT: 0.10.0\n"]},{"output_type":"execute_result","data":{"text/plain":["<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y34QVbQ869Hp","executionInfo":{"status":"ok","timestamp":1764965017747,"user_tz":480,"elapsed":737,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"c2db2503-41ad-446e-dae8-8f6b353dab83"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset from Google Drive into a pandas DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PhD_Thesis_Experiments/GitHub_ToChair/FilteredDataSetForExp_002.csv')"],"metadata":{"id":"l18I9Xye69E8","executionInfo":{"status":"ok","timestamp":1764965020264,"user_tz":480,"elapsed":1341,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","# ================================\n","# 1) Install compatible packages\n","# ================================\n","# !pip -q install \"transformers==4.44.2\" \"datasets==2.21.0\" \"peft==0.10.0\" \"accelerate==0.34.2\" \"scikit-learn==1.5.2\"\n","\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","import torch\n","from datasets import Dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# For reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","# ==========================================\n","# 2) Make sure df is present and normalized\n","# ==========================================\n","# df must have columns: consumer_complaint_narrative, hierarchical_label\n","\n","assert 'consumer_complaint_narrative' in df.columns and 'hierarchical_label' in df.columns, \\\n","    \"DataFrame must have columns: consumer_complaint_narrative, hierarchical_label\"\n","\n","df = df[['consumer_complaint_narrative', 'hierarchical_label']].copy()\n","df.dropna(subset=['consumer_complaint_narrative', 'hierarchical_label'], inplace=True)\n","df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].astype(str).str.strip()\n","df['hierarchical_label'] = df['hierarchical_label'].astype(str).str.strip()\n","\n","# Ensure labels are \"product::sub-product\"; fill missing sub-product with 'None'\n","def normalize_hier_label(label: str):\n","    label = label.strip()\n","    if '::' in label:\n","        p = label.split('::', 1)\n","        product = p[0].strip()\n","        sub = p[1].strip() if p[1].strip() else 'None'\n","        return f\"{product}::{sub}\"\n","    else:\n","        return f\"{label}::None\"\n","\n","df['hierarchical_label'] = df['hierarchical_label'].apply(normalize_hier_label)\n","\n","# ==========================================================\n","# 3) Your Hierarchical Metric Functions (exactly as provided)\n","# ==========================================================\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","# Function to split hierarchical labels into product and sub-product\n","def split_hierarchical_label(label):\n","    if '::' in label:\n","        return label.split('::')\n","    else:\n","        return [label, 'None'] # Handle cases with no sub-product\n","\n","# Function to calculate hierarchical metrics\n","def hierarchical_metrics(y_true, y_pred):\n","    product_true = [split_hierarchical_label(label)[0] for label in y_true]\n","    sub_product_true = [split_hierarchical_label(label)[1] for label in y_true]\n","    product_pred = [split_hierarchical_label(label)[0] for label in y_pred]\n","    sub_product_pred = [split_hierarchical_label(label)[1] for label in y_pred]\n","\n","    # Calculate metrics at the product level\n","    product_precision = precision_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_recall = recall_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_f1 = f1_score(product_true, product_pred, average='weighted', zero_division=0)\n","\n","    # Calculate metrics at the sub-product level (only for non-None sub-products)\n","    valid_sub_product_true = [sub for i, sub in enumerate(sub_product_true) if sub != 'None' and sub_product_pred[i] != 'None']\n","    valid_sub_product_pred = [sub for i, sub in enumerate(sub_product_pred) if sub != 'None' and sub_product_true[i] != 'None']\n","\n","    sub_product_precision = precision_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_recall = recall_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_f1 = f1_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","\n","    # Average product and sub-product metrics\n","    hierarchical_precision = (product_precision + sub_product_precision) / 2\n","    hierarchical_recall = (product_recall + sub_product_recall) / 2\n","    hierarchical_f1 = (product_f1 + sub_product_f1) / 2\n","\n","    return {\n","        'product_precision': product_precision,\n","        'product_recall': product_recall,\n","        'product_f1': product_f1,\n","        'sub_product_precision': sub_product_precision,\n","        'sub_product_recall': sub_product_recall,\n","        'sub_product_f1': sub_product_f1,\n","        'hierarchical_precision': hierarchical_precision,\n","        'hierarchical_recall': hierarchical_recall,\n","        'hierarchical_f1': hierarchical_f1\n","    }\n","\n","# ==========================================================\n","# 4) Build label space and datasets\n","# ==========================================================\n","labels = sorted(df['hierarchical_label'].unique())\n","label2id = {lbl: i for i, lbl in enumerate(labels)}\n","id2label = {i: lbl for lbl, i in label2id.items()}\n","num_labels = len(labels)\n","\n","print(f\"Num classes (hierarchical_label): {num_labels}\")\n","print(\"Sample classes:\", labels[:10])\n","\n","df['label_id'] = df['hierarchical_label'].map(label2id)\n","\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=seed, stratify=df['label_id'])\n","\n","# Tokenizer\n","model_name = 'gpt2'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","max_length = 512  # adjust as needed (up to 1024 for GPT-2)\n","\n","def tokenize_function(batch):\n","    return tokenizer(\n","        batch['consumer_complaint_narrative'],\n","        truncation=True,\n","        max_length=max_length,\n","        padding='max_length'\n","    )\n","\n","train_ds = Dataset.from_pandas(train_df[['consumer_complaint_narrative', 'label_id']])\n","val_ds = Dataset.from_pandas(val_df[['consumer_complaint_narrative', 'label_id']])\n","\n","train_ds = train_ds.map(tokenize_function, batched=True)\n","val_ds = val_ds.map(tokenize_function, batched=True)\n","\n","train_ds = train_ds.rename_column('label_id', 'labels')\n","val_ds = val_ds.rename_column('label_id', 'labels')\n","\n","train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ==========================================================\n","# 5) Precision settings: bf16 if available, else fp32\n","# ==========================================================\n","def gpu_supports_bf16():\n","    if not torch.cuda.is_available():\n","        return False\n","    major, minor = torch.cuda.get_device_capability()\n","    return major >= 8  # Ampere (A100) or newer\n","\n","use_bf16 = gpu_supports_bf16()\n","use_fp16 = False  # Keep fp16 disabled to avoid GradScaler unscale error\n","\n","print(f\"GPU available: {torch.cuda.is_available()}, BF16 support: {use_bf16}, FP16 enabled: {use_fp16}\")\n","\n","# Prefer new TF32 API for speed (optional)\n","try:\n","    torch.backends.cuda.matmul.fp32_precision = \"tf32\"   # 'ieee' or 'tf32'\n","    torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n","except Exception as e:\n","    pass\n","\n","# ==========================================================\n","# 6) Model + LoRA (GPT-2 uses Conv1D; fan_in_fan_out=True)\n","# ==========================================================\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_labels,\n","       torch_dtype=(torch.bfloat16 if use_bf16 else torch.float32)\n",")\n","model.config.pad_token_id = tokenizer.pad_token_id\n","model.config.use_cache = False  # disable cache for training\n","\n","peft_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=['c_attn', 'c_proj'],  # GPT-2 attention QKV + output projection\n","    fan_in_fan_out=True                   # IMPORTANT for GPT-2 Conv1D orientation\n",")\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","# ==========================================================\n","# 7) Trainer setup (will still print a deprecation warning; OK on 4.44.x)\n","# ==========================================================\n","def compute_metrics(eval_pred):\n","    logits, labels_np = eval_pred\n","    preds_np = np.argmax(logits, axis=-1)\n","\n","    # Map IDs to hierarchical strings\n","    y_true_str = [id2label[int(i)] for i in labels_np]\n","    y_pred_str = [id2label[int(i)] for i in preds_np]\n","\n","    # Your hierarchical metrics\n","    h = hierarchical_metrics(y_true_str, y_pred_str)\n","\n","    # Also include standard metrics on the flat label IDs\n","    acc = accuracy_score(labels_np, preds_np)\n","    macro_precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_f1 = f1_score(labels_np, preds_np, average='macro', zero_division=0)\n","    weighted_f1 = f1_score(labels_np, preds_np, average='weighted', zero_division=0)\n","\n","    return {\n","        'accuracy': float(acc),\n","        'macro_precision': float(macro_precision),\n","        'macro_recall': float(macro_recall),\n","        'macro_f1': float(macro_f1),\n","        'weighted_f1': float(weighted_f1),\n","        **{k: float(v) for k, v in h.items()}\n","    }\n","\n","output_dir = '/content/gpt2_lora_cls'\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-4,\n","    warmup_ratio=0.1,\n","    weight_decay=0.01,\n","\n","    evaluation_strategy='epoch',  # deprecation warning OK on 4.44.x\n","    logging_strategy='steps',\n","    logging_steps=50,\n","    save_strategy='epoch',\n","\n","    load_best_model_at_end=True,\n","    metric_for_best_model='eval_hierarchical_f1',\n","    greater_is_better=True,\n","\n","    remove_unused_columns=False,\n","    report_to='none',\n","\n","    fp16=use_fp16,\n","    bf16=use_bf16,\n","    tf32=True\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","# ==========================================================\n","# 8) Train\n","# ==========================================================\n","trainer.train()\n","\n","# ==========================================================\n","# 9) Evaluate using YOUR hierarchical metrics explicitly\n","# ==========================================================\n","eval_preds = trainer.predict(val_ds)\n","pred_ids = np.argmax(eval_preds.predictions, axis=-1)\n","true_ids = eval_preds.label_ids\n","\n","y_true_str = [id2label[int(i)] for i in true_ids]\n","y_pred_str = [id2label[int(i)] for i in pred_ids]\n","hier_eval = hierarchical_metrics(y_true_str, y_pred_str)\n","\n","print(\"\\n=== Hierarchical Evaluation (YOUR functions) ===\")\n","for k, v in hier_eval.items():\n","    print(f\"{k}: {float(v):.4f}\")\n","\n","# Also print a flat classification report over hierarchical label strings\n","print(\"\\n=== Flat Classification Report (over full hierarchical labels) ===\")\n","print(classification_report(true_ids, pred_ids, target_names=[id2label[i] for i in range(num_labels)], zero_division=0))\n","\n","# ==========================================================\n","# 10) Save LoRA adapter + tokenizer, and an inference helper\n","# ==========================================================\n","trainer.save_model(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"\\nModel and tokenizer saved to: {output_dir}\")\n","\n","def predict_label(texts):\n","    if isinstance(texts, str):\n","        texts = [texts]\n","    enc = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n","    enc = {k: v.to(model.device) for k, v in enc.items()}\n","    model.eval()\n","    with torch.no_grad():\n","        logits = model(**enc).logits\n","    preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n","    return [id2label[p] for p in preds]\n","\n","# Example:\n","# sample_text = \"My credit card was charged twice and customer service refused to refund.\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["097d5018cf534d9c86d4f546b305134c","9c81e88a04984dacba64154eb88c35e3","8bb2ed78739e4493be67d62a4a06ed4e","0eb2122bb222433fbd21b4bddcde2312","c1bcb985e325491cbf1d30c93c6ed2c3","7f736f2ce6b2404fa0b3d73824b182b5","802942b3aa7942fc90d50ee85b06f89f","3b05e5b963e7433895058ba6584a90d9","af33cbe416484b8091dd044e07a18e01","e987b18b42a24106be1b027224e4d078","1ac9cb0f813b4d99b773f0c040b3abad","4a2e3cfcca824e6a9516df7c1d50a7ef","135272068ab9410ea6df4e1abe75b3ff","a4986573ebaf4aac9f89500bae4b7d37","08cec16073664abba7974845f5ecf039","a511a3a2253648e49a782b2555b86a46","1988073f251b49beadd4ca41f2803dbb","40160661f42a419ba28ca8aafb31f12e","ebfe7cd60c9442c0b79e1e2503f69ede","79adc24473184ce5820a634d36dbcc99","3a55a9b6488a4729bc498238c67714ec","a7280092f81e427a874db17d87ac0831"]},"id":"_hEv9rRKJaJo","executionInfo":{"status":"ok","timestamp":1764968329714,"user_tz":480,"elapsed":2273623,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"16557aca-efd1-4865-94fe-128d9929a9e8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Num classes (hierarchical_label): 54\n","Sample classes: ['CheckingSavings::cdCertificateOfDeposit', 'CheckingSavings::checkingAccount', 'CheckingSavings::otherBankingProductOrService', 'CheckingSavings::savingsAccount', 'CreditCard::general-purposeCreditCardOrChargeCard', 'CreditCard::storeCreditCard', 'CreditPrepaidCard::general-purposeCreditCardOrChargeCard', 'CreditPrepaidCard::storeCreditCard', 'CreditReporting::creditReporting', 'CreditReporting::otherPersonalConsumerReport']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/86400 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"097d5018cf534d9c86d4f546b305134c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/21600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a2e3cfcca824e6a9516df7c1d50a7ef"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["GPU available: True, BF16 support: True, FP16 enabled: False\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 852,480 || all params: 125,333,760 || trainable%: 0.6801678973007751\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='32400' max='32400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32400/32400 36:15, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Macro F1</th>\n","      <th>Weighted F1</th>\n","      <th>Product Precision</th>\n","      <th>Product Recall</th>\n","      <th>Product F1</th>\n","      <th>Sub Product Precision</th>\n","      <th>Sub Product Recall</th>\n","      <th>Sub Product F1</th>\n","      <th>Hierarchical Precision</th>\n","      <th>Hierarchical Recall</th>\n","      <th>Hierarchical F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.765400</td>\n","      <td>1.790503</td>\n","      <td>0.476852</td>\n","      <td>0.496218</td>\n","      <td>0.476852</td>\n","      <td>0.471406</td>\n","      <td>0.471406</td>\n","      <td>0.717302</td>\n","      <td>0.707037</td>\n","      <td>0.708108</td>\n","      <td>0.507365</td>\n","      <td>0.495972</td>\n","      <td>0.487625</td>\n","      <td>0.612334</td>\n","      <td>0.601505</td>\n","      <td>0.597867</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.739500</td>\n","      <td>1.652695</td>\n","      <td>0.516481</td>\n","      <td>0.524509</td>\n","      <td>0.516481</td>\n","      <td>0.508867</td>\n","      <td>0.508867</td>\n","      <td>0.742788</td>\n","      <td>0.733194</td>\n","      <td>0.736005</td>\n","      <td>0.537448</td>\n","      <td>0.532731</td>\n","      <td>0.524057</td>\n","      <td>0.640118</td>\n","      <td>0.632963</td>\n","      <td>0.630031</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.740700</td>\n","      <td>1.652534</td>\n","      <td>0.514537</td>\n","      <td>0.525832</td>\n","      <td>0.514537</td>\n","      <td>0.509390</td>\n","      <td>0.509390</td>\n","      <td>0.741076</td>\n","      <td>0.733333</td>\n","      <td>0.735671</td>\n","      <td>0.539414</td>\n","      <td>0.531111</td>\n","      <td>0.525031</td>\n","      <td>0.640245</td>\n","      <td>0.632222</td>\n","      <td>0.630351</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","=== Hierarchical Evaluation (YOUR functions) ===\n","product_precision: 0.7411\n","product_recall: 0.7333\n","product_f1: 0.7357\n","sub_product_precision: 0.5394\n","sub_product_recall: 0.5311\n","sub_product_f1: 0.5250\n","hierarchical_precision: 0.6402\n","hierarchical_recall: 0.6322\n","hierarchical_f1: 0.6304\n","\n","=== Flat Classification Report (over full hierarchical labels) ===\n","                                                            precision    recall  f1-score   support\n","\n","                   CheckingSavings::cdCertificateOfDeposit       0.80      0.69      0.74       400\n","                          CheckingSavings::checkingAccount       0.36      0.70      0.48       400\n","             CheckingSavings::otherBankingProductOrService       0.16      0.08      0.11       400\n","                           CheckingSavings::savingsAccount       0.60      0.57      0.58       400\n","         CreditCard::general-purposeCreditCardOrChargeCard       0.44      0.55      0.49       400\n","                               CreditCard::storeCreditCard       0.47      0.55      0.51       400\n","  CreditPrepaidCard::general-purposeCreditCardOrChargeCard       0.67      0.44      0.53       400\n","                        CreditPrepaidCard::storeCreditCard       0.64      0.91      0.75       400\n","                          CreditReporting::creditReporting       0.30      0.40      0.34       400\n","              CreditReporting::otherPersonalConsumerReport       0.38      0.32      0.35       400\n","                    CreditReportingRepair::creditReporting       0.39      0.53      0.45       400\n","        CreditReportingRepair::otherPersonalConsumerReport       0.65      0.76      0.70       400\n","                                  DebtCollection::autoDebt       0.35      0.18      0.24       400\n","                            DebtCollection::creditCardDebt       0.28      0.35      0.31       400\n","                    DebtCollection::federalStudentLoanDebt       0.45      0.42      0.43       400\n","                                DebtCollection::iDoNotKnow       0.40      0.49      0.44       400\n","                               DebtCollection::medicalDebt       0.66      0.57      0.61       400\n","                              DebtCollection::mortgageDebt       0.47      0.47      0.47       400\n","                                 DebtCollection::otherDebt       0.24      0.21      0.22       400\n","                            DebtCollection::paydayLoanDebt       0.29      0.27      0.28       400\n","                    DebtCollection::privateStudentLoanDebt       0.52      0.28      0.37       400\n","                                DebtCollection::rentalDebt       0.59      0.52      0.55       400\n","                    DebtCollection::telecommunicationsDebt       0.30      0.34      0.32       400\n","                      DebtManagement::creditRepairServices       0.34      0.44      0.38       400\n","                            DebtManagement::debtSettlement       0.48      0.28      0.35       400\n","DebtManagement::mortgageModificationOrForeclosureAvoidance       0.73      0.80      0.77       400\n","                        MoneyTransfer::checkCashingService       0.58      0.51      0.55       400\n","                    MoneyTransfer::domesticUsMoneyTransfer       0.89      0.84      0.86       400\n","                    MoneyTransfer::foreignCurrencyExchange       0.76      0.52      0.62       400\n","                 MoneyTransfer::internationalMoneyTransfer       0.59      0.57      0.58       400\n","                      MoneyTransfer::mobileOrDigitalWallet       0.43      0.75      0.55       400\n","    MoneyTransfer::moneyOrderTravelersCheckOrCashiersCheck       0.53      0.53      0.53       400\n","                            MoneyTransfer::virtualCurrency       0.64      0.53      0.58       400\n","                        Mortgage::conventionalHomeMortgage       0.47      0.87      0.61       400\n","                                     Mortgage::fhaMortgage       0.22      0.22      0.22       400\n","               Mortgage::homeEquityLoanOrLineOfCreditHeloc       0.68      0.50      0.58       400\n","                            Mortgage::manufacturedHomeLoan       0.63      0.48      0.55       400\n","                             Mortgage::otherTypeOfMortgage       0.43      0.26      0.33       400\n","                                 Mortgage::reverseMortgage       0.80      0.80      0.80       400\n","                                    Mortgage::usdaMortgage       0.76      0.54      0.63       400\n","                                      Mortgage::vaMortgage       0.58      0.51      0.54       400\n","                               PaydayLoan::installmentLoan       0.27      0.39      0.32       400\n","                   PaydayLoan::otherAdvancesOfFutureIncome       0.66      0.68      0.67       400\n","                                    PaydayLoan::paydayLoan       0.39      0.30      0.34       400\n","                          PaydayLoan::personalLineOfCredit       0.37      0.19      0.25       400\n","                                     PaydayLoan::titleLoan       0.75      0.57      0.65       400\n","                   PrepaidCard::general-purposePrepaidCard       0.48      0.49      0.48       400\n","                                     PrepaidCard::giftCard       0.73      0.63      0.68       400\n","                        PrepaidCard::governmentBenefitCard       0.63      0.53      0.57       400\n","                                  PrepaidCard::payrollCard       0.70      0.68      0.69       400\n","                  StudentLoan::federalStudentLoanServicing       0.57      0.88      0.69       400\n","                           StudentLoan::privateStudentLoan       0.65      0.57      0.61       400\n","                                   VehicleLoanLease::lease       0.71      0.62      0.66       400\n","                                    VehicleLoanLease::loan       0.54      0.70      0.61       400\n","\n","                                                  accuracy                           0.51     21600\n","                                                 macro avg       0.53      0.51      0.51     21600\n","                                              weighted avg       0.53      0.51      0.51     21600\n","\n","\n","Model and tokenizer saved to: /content/gpt2_lora_cls\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LJf4h8Z1JaHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NppGpo3sJaEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XR8BAwcFJaCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_UOnoYsVJZ_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T68wHwNGJZ8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ================================\n","# 1) Install known-compatible libs\n","# ================================\n","# !pip -q install \"transformers==4.44.2\" \"datasets==2.21.0\" \"peft==0.10.0\" \"accelerate==0.34.2\" \"scikit-learn==1.5.2\"\n","\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","\n","from datasets import Dataset\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# For reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","print(\"Torch:\", torch.__version__)\n","import transformers\n","import peft\n","print(\"Transformers:\", transformers.__version__)\n","print(\"PEFT:\", peft.__version__)\n","\n","# ==========================================\n","# 2) Load your data (df or a CSV fallback)\n","# ==========================================\n","# If you already have df in memory with the two columns, skip this block.\n","# Otherwise, uncomment and upload a CSV with the two columns: consumer_complaint_narrative, hierarchical_label\n","\n","# from google.colab import files\n","# uploaded = files.upload()  # pick your CSV file\n","# csv_name = list(uploaded.keys())[0]\n","# df = pd.read_csv(csv_name)\n","\n","assert 'consumer_complaint_narrative' in df.columns and 'hierarchical_label' in df.columns, \\\n","    \"DataFrame must have columns: consumer_complaint_narrative, hierarchical_label\"\n","\n","# Basic cleaning and normalization\n","df = df[['consumer_complaint_narrative', 'hierarchical_label']].copy()\n","df.dropna(subset=['consumer_complaint_narrative', 'hierarchical_label'], inplace=True)\n","df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].astype(str).str.strip()\n","df['hierarchical_label'] = df['hierarchical_label'].astype(str).str.strip()\n","\n","# Ensure labels are \"product::sub-product\" format; fill missing sub-product with 'None'\n","def normalize_hier_label(label: str):\n","    label = label.strip()\n","    if '::' in label:\n","        parts = label.split('::', 1)\n","        product = parts[0].strip()\n","        sub = parts[1].strip() if parts[1].strip() else 'None'\n","        return f\"{product}::{sub}\"\n","    else:\n","        return f\"{label}::None\"\n","\n","df['hierarchical_label'] = df['hierarchical_label'].apply(normalize_hier_label)\n","\n","# ==========================================================\n","# 3) Your Hierarchical Metric Functions (as provided)\n","# ==========================================================\n","def split_hierarchical_label(label):\n","    if '::' in label:\n","        return label.split('::')\n","    else:\n","        return [label, 'None']\n","\n","def hierarchical_metrics(y_true, y_pred):\n","    product_true = [split_hierarchical_label(label)[0] for label in y_true]\n","    sub_product_true = [split_hierarchical_label(label)[1] for label in y_true]\n","\n","    product_pred = [split_hierarchical_label(label)[0] for label in y_pred]\n","    sub_product_pred = [split_hierarchical_label(label)[1] for label in y_pred]\n","\n","    product_precision = precision_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_recall = recall_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_f1 = f1_score(product_true, product_pred, average='weighted', zero_division=0)\n","\n","    valid_sub_product_true = [sub for i, sub in enumerate(sub_product_true) if sub != 'None' and sub_product_pred[i] != 'None']\n","    valid_sub_product_pred = [sub for i, sub in enumerate(sub_product_pred) if sub != 'None' and sub_product_true[i] != 'None']\n","\n","    sub_product_precision = precision_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_recall = recall_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_f1 = f1_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","\n","    hierarchical_precision = (product_precision + sub_product_precision) / 2\n","    hierarchical_recall = (product_recall + sub_product_recall) / 2\n","    hierarchical_f1 = (product_f1 + sub_product_f1) / 2\n","\n","    return {\n","        'product_precision': product_precision,\n","        'product_recall': product_recall,\n","        'product_f1': product_f1,\n","        'sub_product_precision': sub_product_precision,\n","        'sub_product_recall': sub_product_recall,\n","        'sub_product_f1': sub_product_f1,\n","        'hierarchical_precision': hierarchical_precision,\n","        'hierarchical_recall': hierarchical_recall,\n","        'hierarchical_f1': hierarchical_f1\n","    }\n","\n","# ==========================================================\n","# 4) Build label space (full hierarchical strings)\n","# ==========================================================\n","labels = sorted(df['hierarchical_label'].unique())\n","label2id = {lbl: i for i, lbl in enumerate(labels)}\n","id2label = {i: lbl for lbl, i in label2id.items()}\n","num_labels = len(labels)\n","\n","print(f\"Num classes (hierarchical_label): {num_labels}\")\n","print(\"Sample classes:\", labels[:10])\n","\n","# Assign numeric labels\n","df['label_id'] = df['hierarchical_label'].map(label2id)\n","\n","# Train/Validation split (stratified)\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=seed, stratify=df['label_id'])\n","\n","# ==========================================================\n","# 5) Tokenizer and datasets\n","# ==========================================================\n","model_name = 'gpt2'  # or 'gpt2-medium' if you have more memory\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# GPT-2 doesn't have a pad token by default; use EOS as padding\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","max_length = 512  # up to 1024 for GPT-2; watch memory\n","\n","def tokenize_function(batch):\n","    return tokenizer(\n","        batch['consumer_complaint_narrative'],\n","        truncation=True,\n","        max_length=max_length,\n","        padding='max_length'\n","    )\n","\n","train_ds = Dataset.from_pandas(train_df[['consumer_complaint_narrative', 'label_id']])\n","val_ds = Dataset.from_pandas(val_df[['consumer_complaint_narrative', 'label_id']])\n","\n","train_ds = train_ds.map(tokenize_function, batched=True)\n","val_ds = val_ds.map(tokenize_function, batched=True)\n","\n","# Rename 'label_id' to 'labels' for Trainer\n","train_ds = train_ds.rename_column('label_id', 'labels')\n","val_ds = val_ds.rename_column('label_id', 'labels')\n","\n","# Set format to PyTorch tensors\n","train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# ==========================================================\n","# 6) Load GPT-2 for seq classification + LoRA\n","# ==========================================================\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_labels,\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",")\n","\n","# Ensure pad token id is set\n","model.config.pad_token_id = tokenizer.pad_token_id\n","# Disable cache during training to avoid warnings\n","model.config.use_cache = False\n","\n","# LoRA config for GPT-2 (Conv1D-backed linear layers => fan_in_fan_out=True)\n","peft_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=['c_attn', 'c_proj'],  # GPT-2 attention QKV + output proj\n","    fan_in_fan_out=True\n",")\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ==========================================================\n","# 7) Metrics (standard + hierarchical)\n","# ==========================================================\n","def compute_metrics(eval_pred):\n","    logits, labels_np = eval_pred\n","    preds_np = np.argmax(logits, axis=-1)\n","\n","    # Standard metrics\n","    acc = accuracy_score(labels_np, preds_np)\n","    macro_precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_f1 = f1_score(labels_np, preds_np, average='macro', zero_division=0)\n","    weighted_f1 = f1_score(labels_np, preds_np, average='weighted', zero_division=0)\n","\n","    # Hierarchical metrics using your functions\n","    y_true_str = [id2label[int(i)] for i in labels_np]\n","    y_pred_str = [id2label[int(i)] for i in preds_np]\n","    h_metrics = hierarchical_metrics(y_true_str, y_pred_str)\n","\n","    # Combine (plain Python floats)\n","    return {\n","        'accuracy': float(acc),\n","        'macro_precision': float(macro_precision),\n","        'macro_recall': float(macro_recall),\n","        'macro_f1': float(macro_f1),\n","        'weighted_f1': float(weighted_f1),\n","        **{k: float(v) for k, v in h_metrics.items()}\n","    }\n","\n","# ==========================================================\n","# 8) Training\n","# ==========================================================\n","output_dir = '/content/gpt2_lora_cls'\n","\n","# Use fp16 on T4/V100; switch to bf16 if A100\n","use_fp16 = torch.cuda.is_available()\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,   # adjust for memory\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-4,              # slightly higher LR works well with LoRA\n","    warmup_ratio=0.1,\n","    weight_decay=0.01,\n","    evaluation_strategy='epoch',     # <-- fixed: supported by pinned Transformers\n","    logging_strategy='steps',\n","    logging_steps=50,\n","    save_strategy='epoch',\n","    load_best_model_at_end=True,\n","    metric_for_best_model='eval_hierarchical_f1',  # Trainer will log metrics with 'eval_' prefix\n","    greater_is_better=True,\n","    fp16=use_fp16,\n","    remove_unused_columns=False,      # important for decoder-only models\n","    report_to='none'\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n","\n","# ==========================================================\n","# 9) Final Evaluation + Classification Report\n","# ==========================================================\n","eval_res = trainer.evaluate()\n","print(\"\\n=== Final Evaluation (standard + hierarchical) ===\")\n","for k, v in eval_res.items():\n","    try:\n","        print(f\"{k}: {float(v):.4f}\")\n","    except Exception:\n","        print(f\"{k}: {v}\")\n","\n","# Optional: detailed classification report (flat labels)\n","preds = trainer.predict(val_ds)\n","pred_ids = np.argmax(preds.predictions, axis=-1)\n","true_ids = preds.label_ids\n","\n","print(\"\\n=== Flat Classification Report (over full hierarchical labels) ===\")\n","print(classification_report(true_ids, pred_ids, target_names=[id2label[i] for i in range(num_labels)], zero_division=0))\n","\n","# ==========================================================\n","# 10) Save LoRA adapter + tokenizer\n","# ==========================================================\n","trainer.save_model(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(f\"\\nModel and tokenizer saved to: {output_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":787,"referenced_widgets":["8d8d413ef023420bb4da14aa93f0b76b","2c7af6eb0c174d92a8f9a34ba1b895be","bb9cdd85bd2c4fd387f436f954b0a3ea","5b96849fe1824679924e6f38a53edc6a","32fc3c4d81414291bde3cda462d2bb7e","745ed74945b04024826c41b2934a957f","1137cc0cb8824800a94f925f1d531c70","3d67ceea3b5a4578915307610fc0ab23","a352ab13124346148c22e5c023b101e0","3c36645bdb044623b6ab090d90347569","4cbc5c31d83b4d5696389cec4c541364","fd32bde8e284460596b71808fcf5f979","9e7a800eb2d4418989cf8af405252a84","a9556caa97e24be1ada1657b29f1afab","654953f8fc464578b4d8cfa479c0c9e8","84e5eddb05264311a0ad031d620e469e","408074174d064a3e8277acaa8165926d","c102fabeafce4b2180093f4e8d09f260","fdb3c152344a4c2fa3a291e12435f301","5231bc3a689b4dde9fa71174b4eb8567","4e8a6ebd907444938ff136d1595f2c35","9cc042cd702c445cbfd1083a827a9e62"]},"id":"Nu-qOwIbF8z1","executionInfo":{"status":"error","timestamp":1764965081438,"user_tz":480,"elapsed":44608,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"72f1e1f1-0a2c-4954-9297-b02c2b5b0973"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch: 2.9.0+cu126\n","Transformers: 4.44.2\n","PEFT: 0.10.0\n","Num classes (hierarchical_label): 54\n","Sample classes: ['CheckingSavings::cdCertificateOfDeposit', 'CheckingSavings::checkingAccount', 'CheckingSavings::otherBankingProductOrService', 'CheckingSavings::savingsAccount', 'CreditCard::general-purposeCreditCardOrChargeCard', 'CreditCard::storeCreditCard', 'CreditPrepaidCard::general-purposeCreditCardOrChargeCard', 'CreditPrepaidCard::storeCreditCard', 'CreditReporting::creditReporting', 'CreditReporting::otherPersonalConsumerReport']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/86400 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d8d413ef023420bb4da14aa93f0b76b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/21600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd32bde8e284460596b71808fcf5f979"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 852,480 || all params: 125,333,760 || trainable%: 0.6801678973007751\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"error","ename":"ValueError","evalue":"Attempting to unscale FP16 gradients.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3220910378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;31m# ==========================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                             )\n\u001b[1;32m   2324\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m                             _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2344\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2288\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."]}]},{"cell_type":"code","source":[],"metadata":{"id":"5xTMM0gKF8w7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RVo4r3EtF8uV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HF0EZcwXF8r7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k2YqxKoBF8pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Xul1YKvIF8m7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SOZWUvhNF8kL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gsWB-F1EF8hS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-6_2_4dJF8ei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"],"metadata":{"id":"7tOtyl2_7lUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    GPT2TokenizerFast,\n","    GPT2ForSequenceClassification,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n"],"metadata":{"id":"gK04Hwmr7lR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# For reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"Qva19W6J7lPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Basic cleaning and normalization\n","df = df[['consumer_complaint_narrative', 'hierarchical_label']].copy()\n","df.dropna(subset=['consumer_complaint_narrative', 'hierarchical_label'], inplace=True)\n","df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].astype(str).str.strip()\n","df['hierarchical_label'] = df['hierarchical_label'].astype(str).str.strip()\n"],"metadata":{"id":"3wNEPHcA7lNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure labels are \"product::sub-product\" format; fill missing sub-product with 'None'\n","def normalize_hier_label(label: str):\n","    label = label.strip()\n","    if '::' in label:\n","        parts = label.split('::', 1)\n","        product = parts[0].strip()\n","        sub = parts[1].strip() if parts[1].strip() else 'None'\n","        return f\"{product}::{sub}\"\n","    else:\n","        return f\"{label}::None\"\n","\n","df['hierarchical_label'] = df['hierarchical_label'].apply(normalize_hier_label)\n"],"metadata":{"id":"W8cAKio37lKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# 3) Your Hierarchical Metric Functions (as provided)\n","# ==========================================================\n","def split_hierarchical_label(label):\n","    if '::' in label:\n","        return label.split('::')\n","    else:\n","        return [label, 'None']\n","\n","def hierarchical_metrics(y_true, y_pred):\n","    product_true = [split_hierarchical_label(label)[0] for label in y_true]\n","    sub_product_true = [split_hierarchical_label(label)[1] for label in y_true]\n","\n","    product_pred = [split_hierarchical_label(label)[0] for label in y_pred]\n","    sub_product_pred = [split_hierarchical_label(label)[1] for label in y_pred]\n","\n","    product_precision = precision_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_recall = recall_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_f1 = f1_score(product_true, product_pred, average='weighted', zero_division=0)\n","\n","    valid_sub_product_true = [sub for i, sub in enumerate(sub_product_true) if sub != 'None' and sub_product_pred[i] != 'None']\n","    valid_sub_product_pred = [sub for i, sub in enumerate(sub_product_pred) if sub != 'None' and sub_product_true[i] != 'None']\n","\n","    sub_product_precision = precision_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_recall = recall_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_f1 = f1_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","\n","    hierarchical_precision = (product_precision + sub_product_precision) / 2\n","    hierarchical_recall = (product_recall + sub_product_recall) / 2\n","    hierarchical_f1 = (product_f1 + sub_product_f1) / 2\n","\n","    return {\n","        'product_precision': product_precision,\n","        'product_recall': product_recall,\n","        'product_f1': product_f1,\n","        'sub_product_precision': sub_product_precision,\n","        'sub_product_recall': sub_product_recall,\n","        'sub_product_f1': sub_product_f1,\n","        'hierarchical_precision': hierarchical_precision,\n","        'hierarchical_recall': hierarchical_recall,\n","        'hierarchical_f1': hierarchical_f1\n","    }\n"],"metadata":{"id":"n6pjcxYd8x5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# 4) Build label space (full hierarchical strings)\n","# ==========================================================\n","labels = sorted(df['hierarchical_label'].unique())\n","label2id = {lbl: i for i, lbl in enumerate(labels)}\n","id2label = {i: lbl for lbl, i in label2id.items()}\n","num_labels = len(labels)\n","\n","print(f\"Num classes (hierarchical_label): {num_labels}\")\n","print(\"Sample classes:\", labels[:10])\n","\n","# Assign numeric labels\n","df['label_id'] = df['hierarchical_label'].map(label2id)\n","\n","# Train/Validation split\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=seed, stratify=df['label_id'])\n","\n","# ==========================================================\n","# 5) Tokenizer and datasets\n","# ==========================================================\n","model_name = 'gpt2'  # you can use 'gpt2-medium' if you have more GPU memory\n","tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n","\n","# GPT-2 doesn't have a pad token by default; use EOS as padding\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","max_length = 512  # adjust as needed (GPT-2 supports up to 1024)\n","\n","def tokenize_function(batch):\n","    return tokenizer(\n","        batch['consumer_complaint_narrative'],\n","        truncation=True,\n","        max_length=max_length,\n","        padding='max_length'\n","    )\n","\n","train_ds = Dataset.from_pandas(train_df[['consumer_complaint_narrative', 'label_id']])\n","val_ds = Dataset.from_pandas(val_df[['consumer_complaint_narrative', 'label_id']])\n","\n","train_ds = train_ds.map(tokenize_function, batched=True)\n","val_ds = val_ds.map(tokenize_function, batched=True)\n","\n","# Rename 'label_id' to 'labels' for Trainer\n","train_ds = train_ds.rename_column('label_id', 'labels')\n","val_ds = val_ds.rename_column('label_id', 'labels')\n","\n","# Set format to PyTorch tensors\n","train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# ==========================================================\n","# 6) Load GPT-2 for sequence classification + LoRA\n","# ==========================================================\n","model = GPT2ForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_labels\n",")\n","\n","# Ensure pad token id is set\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# LoRA config for GPT-2\n","peft_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=['c_attn', 'c_proj']  # common GPT-2 linear layers for attention\n",")\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ==========================================================\n","# 7) Metrics (standard + hierarchical)\n","# ==========================================================\n","def compute_metrics(eval_pred):\n","    logits, labels_np = eval_pred\n","    preds_np = np.argmax(logits, axis=-1)\n","\n","    # Standard metrics\n","    acc = accuracy_score(labels_np, preds_np)\n","    macro_precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_f1 = f1_score(labels_np, preds_np, average='macro', zero_division=0)\n","    weighted_f1 = f1_score(labels_np, preds_np, average='weighted', zero_division=0)\n","\n","    # Hierarchical metrics using your functions\n","    y_true_str = [id2label[int(i)] for i in labels_np]\n","    y_pred_str = [id2label[int(i)] for i in preds_np]\n","    h_metrics = hierarchical_metrics(y_true_str, y_pred_str)\n","\n","    # Combine\n","    return {\n","        'accuracy': acc,\n","        'macro_precision': macro_precision,\n","        'macro_recall': macro_recall,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        **h_metrics\n","    }\n","\n","# ==========================================================\n","# 8) Training\n","# ==========================================================\n","output_dir = '/content/gpt2_lora_cls'\n","\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,   # adjust based on GPU memory\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-4,              # LoRA often allows slightly higher LR\n","    warmup_ratio=0.1,\n","    weight_decay=0.01,\n","    evaluation_strategy='epoch',\n","    logging_strategy='steps',\n","    logging_steps=50,\n","    save_strategy='epoch',\n","    load_best_model_at_end=True,\n","    metric_for_best_model='eval_hierarchical_f1',  # Select best on hierarchical F1\n","    greater_is_better=True,\n","    fp16=True,                        # Colab GPU supports mixed precision\n","    report_to='none'                  # disable W&B etc.\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n","\n","# ==========================================================\n","# 9) Final Evaluation + Classification Report\n","# ==========================================================\n","eval_res = trainer.evaluate()\n","print(\"\\n=== Final Evaluation (standard + hierarchical) ===\")\n","for k, v in eval_res.items():\n","    if isinstance(v, (float, np.floating)):\n","        print(f\"{k}: {v:.4f}\")\n","    else:\n","        print(f\"{k}: {v}\")\n","\n","# Optional: detailed classification report (flat labels)\n","preds = trainer.predict(val_ds)\n","pred_ids = np.argmax(preds.predictions, axis=-1)\n","true_ids = preds.label_ids\n","\n","print(\"\\n=== Flat Classification Report (weighted/macro over full hierarchical labels) ===\")\n","print(classification_report(true_ids, pred_ids, target_names=[id2label[i] for i in range(num_labels)], zero_division=0))\n","\n","# ==========================================================\n","# 10) Save LoRA adapter + tokenizer\n","# ==========================================================\n","trainer.save_model(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(f\"\\nModel and tokenizer saved to: {output_dir}\")\n","\n","# ==========================================================\n","# 11) Inference helper\n","# ==========================================================\n","def predict_label(texts):\n","    if isinstance(texts, str):\n","        texts = [texts]\n","    enc = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n","    enc = {k: v.to(model.device) for k, v in enc.items()}\n","    with torch.no_grad():\n","        logits = model(**enc).logits\n","    preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n","    return [id2label[p] for p in preds]\n","\n","# Example:\n","# sample_text = \"My credit card was charged twice and customer service refused to refund.\"\n","print(predict_label(sample_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712,"referenced_widgets":["e6a33cd7eaae41158af7659a6fe21398","9bbecbe0d7a94c1a8efe3b2fefecf4eb","4d24a7210677463fb505653078de3f01","413b76d9dafd401fb83e545a9ee13e1a","133253024f90460d8e33f285fe710334","fe3ac2d19a644434ae555da88d6e7f45","b0821a1b57694ba290cd7e0b31e9aa19","4a0ee8be5aa24efda2a5b2a8ad03fc4e","f4aa22ac13d542fd9c9d8696be667480","f9a3b4a0d4744554bdfd410fd7bde02e","cdb6fc5244f044a3bbe8aef47c686bb1","dea30ad663f94599be4d31f74b121a2c","76c7add27b7c48c79f492fc6faf80aa4","33836de09a2e4a9ca3f5638fdec3d507","e0b3b232aec648fdbe3c73c91c11a5b6","981bb55f2f524c9d883317c81e3a7e8a","a4281de7adac4751bca14cd31309cda8","73ab33fd5bf74b89869e903992ea6949","8fc1b8fa5d3a4a8c91493bbe1b540755","d9477df8e63c43e7a71ed3cf3d01f3b6","c33b835b934e4a1a943980cd38f52af4","a46a9c89c4cd4b1da7eb849d987c9250","fb8acef63acb4ca9879e0e7da0f702a2","ab3726d5ad9a4b3381a45c2dfb3fbbb8","c80e13758e1547768b7f3ad9b44b0373","6717d1476acc49f39e3b84afcf3696a4","e460e4fdf29d4993af71c22e2cfafc0d","240fe4c0de7d4072bedcb87c0ee18dd4","13c9e794bd7648e687f1a40b6e455680","a98a4961a60e4bedaac38e9269aa105a","973f221d241d49f5a1e19e294d7890fd","9ab3eb93d7ba4f738fb74db85a557c1c","7b585cec79c042c6b5f5006945e4b906","972ab552be514118ada80ee196c5717c","3dd0c5896d5f429ea92cb4f678073f27","37ce8a7d44304c75b43c31b077ff76d7","3d111135ef7d46b6959b85cbd6b8e4f0","3cd8ba6a2dd545ad966c53326de0720f","50eaa21fd4e14a069b7663519e88f644","8343c08eb6754a9e89cd773a04232d9b","b5a001a3fcf344d0bc5886c26cb2111a","356b3da2f78b4e59a883be10140e4e1b","28c971a2d51044f9b35ff2195fef1f7f","0905890f391849179eeba793ff1bd64a","065add95131d4cb0a3c7a84fce1fc2ad","4ae1cb7874be466ea7f2ee1cf3af3de0","2589786454324cc3a0f5dca1614d08e3","356d4055139d46e589f2ed0dc6b69a92","9087a416873a46efb59ba2473c09b6bc","f18b461e0ca24cb1a028692e6902b80e","0acf382d580d41e1b7cb7f1950ee3549","b5b368967a5e4da398ae08b01bcb6478","020534237a814ebf903cbbad8041ae43","60add58d51174a799b89231e5b711705","af37ce4e207b43bcaaf26de31c7b0da4","4949a06dddb54564ac0011933325aa41","619feb8162cd43d59d10f774f1521268","fb0989d125af46f89bce3fce5496bcec","7b8e7ea983504171a5a4fe070eb320df","b3ff300e9b3640b9b38aeaaa3557fc96","9a4b5195693c4ff39dccc47c6cd798e0","125366fa88084b75aa3ca423230266cb","65afebce24ae45449444e1c488d01353","6c77039d803543b9b4ddb539b0d0bc17","e04292c153314398913ca7963f32b695","2841305870fc4bfea30bfe5809333ca8","9a0efefda895467c90a6795e8073d73e","b27e9f3ca6e04253bd436cffbb80a2bb","d9ef7d8b63bb43f194cffe9e4e06ce41","84498bba11c64ceba53ef02cdf8a4f2c","5fd71079058149ed90d64197c57b36d7","70d29925e11341169bff92d57b7e99be","3d59727611f84c1babac815c7847c6ad","93bb00c0b5a141a98b0fd6cc54692dc5","1718d486847a45f39a07dff46b02e2fb","3e68462735104ed6a7b0c4c16030a6ea","35d60a1253ca4428992be1b0929e9c41","c50d52540fd5479d9724ee5e387ba3a7","f88b86a07d844e0aad170316ac49400d","ddebe51be0954aa2896534af9123e949","c0f61a06773d41c3ac39f29c3b6f7549","d8b2b3a4da0041078fed1538bc12e367","1e208a4a20954170b6d7bc65c472b11a","4d8cdc0de9374e5a8de4d753b359a3c5","6c5f6f29478f418abcf93a96e1385584","1f9fedeb5bbf4d29a5ce1b1bd89644ef","3004d4a8671e4963b0b1e52e42734e50","577f6976c84a45b3a5038f2e7c56a99f"]},"id":"Bfcq0qnw8x2d","executionInfo":{"status":"error","timestamp":1764962608794,"user_tz":480,"elapsed":189978,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"ee1963cb-40ab-4f5c-8608-4c74a4455f35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Num classes (hierarchical_label): 54\n","Sample classes: ['CheckingSavings::cdCertificateOfDeposit', 'CheckingSavings::checkingAccount', 'CheckingSavings::otherBankingProductOrService', 'CheckingSavings::savingsAccount', 'CreditCard::general-purposeCreditCardOrChargeCard', 'CreditCard::storeCreditCard', 'CreditPrepaidCard::general-purposeCreditCardOrChargeCard', 'CreditPrepaidCard::storeCreditCard', 'CreditReporting::creditReporting', 'CreditReporting::otherPersonalConsumerReport']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a33cd7eaae41158af7659a6fe21398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dea30ad663f94599be4d31f74b121a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8acef63acb4ca9879e0e7da0f702a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972ab552be514118ada80ee196c5717c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065add95131d4cb0a3c7a84fce1fc2ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/86400 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4949a06dddb54564ac0011933325aa41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/21600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a0efefda895467c90a6795e8073d73e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50d52540fd5479d9724ee5e387ba3a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 852,480 || all params: 125,333,760 || trainable%: 0.6802\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-736150437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gpt2_lora_cls'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"]}]},{"cell_type":"code","source":["\n","# ================================\n","# 1) Install known-compatible libs\n","# ================================\n","# !pip -q install \"transformers==4.44.2\" \"datasets==2.21.0\" \"peft==0.10.0\" \"accelerate==0.34.2\" \"scikit-learn==1.5.2\"\n","\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    Trainer,\n","    TrainingArguments,\n",")\n","\n","from datasets import Dataset\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# For reproducibility\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","print(\"Torch:\", torch.__version__)\n","import transformers\n","import peft\n","print(\"Transformers:\", transformers.__version__)\n","print(\"PEFT:\", peft.__version__)\n","\n","# ==========================================\n","# 2) Load your data (df or a CSV fallback)\n","# ==========================================\n","# If you already have df in memory with the two columns, skip this block.\n","# Otherwise, uncomment and upload a CSV with the two columns: consumer_complaint_narrative, hierarchical_label\n","\n","# from google.colab import files\n","# uploaded = files.upload()  # pick your CSV file\n","# csv_name = list(uploaded.keys())[0]\n","# df = pd.read_csv(csv_name)\n","\n","assert 'consumer_complaint_narrative' in df.columns and 'hierarchical_label' in df.columns, \\\n","    \"DataFrame must have columns: consumer_complaint_narrative, hierarchical_label\"\n","\n","# Basic cleaning and normalization\n","df = df[['consumer_complaint_narrative', 'hierarchical_label']].copy()\n","df.dropna(subset=['consumer_complaint_narrative', 'hierarchical_label'], inplace=True)\n","df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].astype(str).str.strip()\n","df['hierarchical_label'] = df['hierarchical_label'].astype(str).str.strip()\n","\n","# Ensure labels are \"product::sub-product\" format; fill missing sub-product with 'None'\n","def normalize_hier_label(label: str):\n","    label = label.strip()\n","    if '::' in label:\n","        parts = label.split('::', 1)\n","        product = parts[0].strip()\n","        sub = parts[1].strip() if parts[1].strip() else 'None'\n","        return f\"{product}::{sub}\"\n","    else:\n","        return f\"{label}::None\"\n","\n","df['hierarchical_label'] = df['hierarchical_label'].apply(normalize_hier_label)\n","\n","# ==========================================================\n","# 3) Your Hierarchical Metric Functions (as provided)\n","# ==========================================================\n","def split_hierarchical_label(label):\n","    if '::' in label:\n","        return label.split('::')\n","    else:\n","        return [label, 'None']\n","\n","def hierarchical_metrics(y_true, y_pred):\n","    product_true = [split_hierarchical_label(label)[0] for label in y_true]\n","    sub_product_true = [split_hierarchical_label(label)[1] for label in y_true]\n","\n","    product_pred = [split_hierarchical_label(label)[0] for label in y_pred]\n","    sub_product_pred = [split_hierarchical_label(label)[1] for label in y_pred]\n","\n","    product_precision = precision_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_recall = recall_score(product_true, product_pred, average='weighted', zero_division=0)\n","    product_f1 = f1_score(product_true, product_pred, average='weighted', zero_division=0)\n","\n","    valid_sub_product_true = [sub for i, sub in enumerate(sub_product_true) if sub != 'None' and sub_product_pred[i] != 'None']\n","    valid_sub_product_pred = [sub for i, sub in enumerate(sub_product_pred) if sub != 'None' and sub_product_true[i] != 'None']\n","\n","    sub_product_precision = precision_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_recall = recall_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","    sub_product_f1 = f1_score(valid_sub_product_true, valid_sub_product_pred, average='weighted', zero_division=0) if valid_sub_product_true else 0\n","\n","    hierarchical_precision = (product_precision + sub_product_precision) / 2\n","    hierarchical_recall = (product_recall + sub_product_recall) / 2\n","    hierarchical_f1 = (product_f1 + sub_product_f1) / 2\n","\n","    return {\n","        'product_precision': product_precision,\n","        'product_recall': product_recall,\n","        'product_f1': product_f1,\n","        'sub_product_precision': sub_product_precision,\n","        'sub_product_recall': sub_product_recall,\n","        'sub_product_f1': sub_product_f1,\n","        'hierarchical_precision': hierarchical_precision,\n","        'hierarchical_recall': hierarchical_recall,\n","        'hierarchical_f1': hierarchical_f1\n","    }\n","\n","# ==========================================================\n","# 4) Build label space (full hierarchical strings)\n","# ==========================================================\n","labels = sorted(df['hierarchical_label'].unique())\n","label2id = {lbl: i for i, lbl in enumerate(labels)}\n","id2label = {i: lbl for lbl, i in label2id.items()}\n","num_labels = len(labels)\n","\n","print(f\"Num classes (hierarchical_label): {num_labels}\")\n","print(\"Sample classes:\", labels[:10])\n","\n","# Assign numeric labels\n","df['label_id'] = df['hierarchical_label'].map(label2id)\n","\n","# Train/Validation split (stratified)\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=seed, stratify=df['label_id'])\n","\n","# ==========================================================\n","# 5) Tokenizer and datasets\n","# ==========================================================\n","model_name = 'gpt2'  # or 'gpt2-medium' if you have more memory\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# GPT-2 doesn't have a pad token by default; use EOS as padding\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","max_length = 512  # up to 1024 for GPT-2; watch memory\n","\n","def tokenize_function(batch):\n","    return tokenizer(\n","        batch['consumer_complaint_narrative'],\n","        truncation=True,\n","        max_length=max_length,\n","        padding='max_length'\n","    )\n","\n","train_ds = Dataset.from_pandas(train_df[['consumer_complaint_narrative', 'label_id']])\n","val_ds = Dataset.from_pandas(val_df[['consumer_complaint_narrative', 'label_id']])\n","\n","train_ds = train_ds.map(tokenize_function, batched=True)\n","val_ds = val_ds.map(tokenize_function, batched=True)\n","\n","# Rename 'label_id' to 'labels' for Trainer\n","train_ds = train_ds.rename_column('label_id', 'labels')\n","val_ds = val_ds.rename_column('label_id', 'labels')\n","\n","# Set format to PyTorch tensors\n","train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# ==========================================================\n","# 6) Load GPT-2 for seq classification + LoRA\n","# ==========================================================\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_labels,\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",")\n","\n","# Ensure pad token id is set\n","model.config.pad_token_id = tokenizer.pad_token_id\n","# Disable cache during training to avoid warnings\n","model.config.use_cache = False\n","\n","# LoRA config for GPT-2 (Conv1D-backed linear layers => fan_in_fan_out=True)\n","peft_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=['c_attn', 'c_proj'],  # GPT-2 attention QKV + output proj\n","    fan_in_fan_out=True\n",")\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ==========================================================\n","# 7) Metrics (standard + hierarchical)\n","# ==========================================================\n","def compute_metrics(eval_pred):\n","    logits, labels_np = eval_pred\n","    preds_np = np.argmax(logits, axis=-1)\n","\n","    # Standard metrics\n","    acc = accuracy_score(labels_np, preds_np)\n","    macro_precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)\n","    macro_f1 = f1_score(labels_np, preds_np, average='macro', zero_division=0)\n","    weighted_f1 = f1_score(labels_np, preds_np, average='weighted', zero_division=0)\n","\n","    # Hierarchical metrics using your functions\n","    y_true_str = [id2label[int(i)] for i in labels_np]\n","    y_pred_str = [id2label[int(i)] for i in preds_np]\n","    h_metrics = hierarchical_metrics(y_true_str, y_pred_str)\n","\n","    # Combine (plain Python floats)\n","    return {\n","        'accuracy': float(acc),\n","        'macro_precision': float(macro_precision),\n","        'macro_recall': float(macro_recall),\n","        'macro_f1': float(macro_f1),\n","        'weighted_f1': float(weighted_f1),\n","        **{k: float(v) for k, v in h_metrics.items()}\n","    }\n","\n","# ==========================================================\n","# 8) Training\n","# ==========================================================\n","output_dir = '/content/gpt2_lora_cls'\n","\n","# Use fp16 on T4/V100; switch to bf16 if A100\n","use_fp16 = torch.cuda.is_available()\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,   # adjust for memory\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-4,              # slightly higher LR works well with LoRA\n","    warmup_ratio=0.1,\n","    weight_decay=0.01,\n","    evaluation_strategy='epoch',     # <-- fixed: supported by pinned Transformers\n","    logging_strategy='steps',\n","    logging_steps=50,\n","    save_strategy='epoch',\n","    load_best_model_at_end=True,\n","    metric_for_best_model='eval_hierarchical_f1',  # Trainer will log metrics with 'eval_' prefix\n","    greater_is_better=True,\n","    fp16=use_fp16,\n","    remove_unused_columns=False,      # important for decoder-only models\n","    report_to='none'\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n","\n","# ==========================================================\n","# 9) Final Evaluation + Classification Report\n","# ==========================================================\n","eval_res = trainer.evaluate()\n","print(\"\\n=== Final Evaluation (standard + hierarchical) ===\")\n","for k, v in eval_res.items():\n","    try:\n","        print(f\"{k}: {float(v):.4f}\")\n","    except Exception:\n","        print(f\"{k}: {v}\")\n","\n","# Optional: detailed classification report (flat labels)\n","preds = trainer.predict(val_ds)\n","pred_ids = np.argmax(preds.predictions, axis=-1)\n","true_ids = preds.label_ids\n","\n","print(\"\\n=== Flat Classification Report (over full hierarchical labels) ===\")\n","print(classification_report(true_ids, pred_ids, target_names=[id2label[i] for i in range(num_labels)], zero_division=0))\n","\n","# ==========================================================\n","# 10) Save LoRA adapter + tokenizer\n","# ==========================================================\n","trainer.save_model(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(f\"\\nModel and tokenizer saved to: {output_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451,"referenced_widgets":["db2a996df3f24ef5ba3059e4d7fe72cd","ef2857e0784045a6aa28dc74a3f34da2","ae3b337aa213415196c1b9e40b39f183","6ce31fa3493f4468aa3efa926b692b8c","27f58541371a4f4d9a8f41997e829121","3902881f89d745c7b3292762c48b2f4b","58b105175e02471c9eadb25291b3412f","4d8057e215e94042a6859508eb4bb3a9","7c2f3b2300cb450ba4a0358419c9a0f6","3bad45fcb81f463aa074fe79faa0ed13","47d0f039535342bea42f492a4401edd2","91089f50b21a49e881a2e4871ec07764","5719f7b6066344bba4434b06ef07efc0","044cde722575441688f5640add7a5b15","d59729e5fba341de8779eff92d28254f","837d8db126a54e95ab0c4caaf356f712","01e8542df9584a00bfc360fbe1f0af86","26438d2c67d74f48b20846ca44a7ddb6","e84a1e6e5a024eb28bcc35302fddb400","7dd55f0501cd4aedb2f91270cce07106","b8fec893f3e2488a8de1cf8047ff46d5","106ca99b75b44af9919b8311c6759726"]},"id":"YzkkEh7t8x0A","executionInfo":{"status":"error","timestamp":1764963925072,"user_tz":480,"elapsed":127213,"user":{"displayName":"Kundan Karma","userId":"01236420441799182112"}},"outputId":"e450a7b9-63e9-4004-fc67-1366ebb1b0de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch: 2.9.0+cu126\n","Transformers: 4.57.3\n","PEFT: 0.18.0\n","Num classes (hierarchical_label): 54\n","Sample classes: ['CheckingSavings::cdCertificateOfDeposit', 'CheckingSavings::checkingAccount', 'CheckingSavings::otherBankingProductOrService', 'CheckingSavings::savingsAccount', 'CreditCard::general-purposeCreditCardOrChargeCard', 'CreditCard::storeCreditCard', 'CreditPrepaidCard::general-purposeCreditCardOrChargeCard', 'CreditPrepaidCard::storeCreditCard', 'CreditReporting::creditReporting', 'CreditReporting::otherPersonalConsumerReport']\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/86400 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db2a996df3f24ef5ba3059e4d7fe72cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/21600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91089f50b21a49e881a2e4871ec07764"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 852,480 || all params: 125,333,760 || trainable%: 0.6802\n"]},{"output_type":"error","ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3220910378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;31m# Use fp16 on T4/V100; switch to bf16 if A100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0muse_fp16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"]}]},{"cell_type":"code","source":["# ==========================================================\n","# 11) Inference helper\n","# ==========================================================\n","def predict_label(texts):\n","    if isinstance(texts, str):\n","        texts = [texts]\n","    enc = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n","    enc = {k: v.to(model.device) for k, v in enc.items()}\n","    model.eval()\n","    with torch.no_grad():\n","      logits = model(**enc).logits\n","    preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n","    return [id2label[p] for p in preds]\n","# Example:\n","# sample_text = \"My credit card was charged twice and customer service refused to refund.\"\n","# Example:\n","# sample_text = \"My credit card was charged twice and customer service refused to refund.\"\n","print(predict_label(sample_text))"],"metadata":{"id":"xzWwCnXP8xxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zdBhDgOE8xu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IqBqcPXd6VVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o1zikC9S6VXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KM4ODgnQ6VaN"},"execution_count":null,"outputs":[]}]}